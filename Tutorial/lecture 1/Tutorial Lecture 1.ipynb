{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph and Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow support Data Flow Graphs\n",
    "\n",
    "# phase 1: assemble a graph\n",
    "a = tf.add(2, 3) # tf automatically name the nodes when you don't explicitly name them\n",
    "# what is node? Nodes: operators, variables, and constants\n",
    "# What is edges: tensors (data)\n",
    "print a # Not 5, so how to get the value of a? Create a session, assign it to variable sess so can call it later\n",
    "        # within the session, evaluate the graph to fetch the value of a (phase 2)\n",
    "\n",
    "# phase 2: use a session to execute operations in the graph\n",
    "sess = tf.Session()  # a Session object encapsulate the environment in which Operation objects are executed, and the Tensor objects are evaluated\n",
    "print sess.run(a) # output the result 5\n",
    "sess.close # free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More graph\n",
    "x = 2\n",
    "y = 3\n",
    "op1 = tf.add(x, y) # 5\n",
    "op2 = tf.multiply(x, y) # 6\n",
    "op3 = tf.pow(op2, op1) # 6 ^ 5\n",
    "with tf.Session() as sess:\n",
    "    op3 = sess.run(op3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It is possible to break graphs into several chunks and run them parallelly across multiple CPUs, GPUs, or devices\n",
    "# Distributed Computation\n",
    "# To put part of a graph on a specific CPU or GPU\n",
    "# Create a graph\n",
    "with tf.device('/cpu:2'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b) # Multiplies matrix a by matrix b, producing a * b, (a, b must be matrix, changed in TF 1.1 version)\n",
    "    \n",
    "# Creates a session with log_device_placement set to True\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# Runs the op.\n",
    "print sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if want more than one graph\n",
    "# multiple graphs require multiple sessions, each will try to use all available resources by default\n",
    "# can't pass data btw them without passing them through python/numpy. which does not work in distributed\n",
    "# better to have disconnected subgraphs within one graph\n",
    "\n",
    "g = tf.Graph() # create a graph\n",
    "with g.as_default(): # add operators to a graph, set it as default\n",
    "    x = tf.add(3, 5)\n",
    "    \n",
    "with tf.Session(graph=g) as sess: # session is run on the graph g; session would be close automatically\n",
    "    print sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not mix default graph and user created graphs\n",
    "g1 = tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "\n",
    "with g1.as_default():\n",
    "    a = tf.constant(3)\n",
    "    \n",
    "with g2.as_default():\n",
    "    b = tf.constant(5)\n",
    "\n",
    "sess = tf.Session() # run on the default graph\n",
    "print sess.run(a) # error if run(b) because default graph does not have node b\n",
    "\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    print sess.run(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why graphs?\n",
    "- save computation, only run subgraphs that lead to the values we want to fetch\n",
    "- Break computation into small, different pieces to facilitates auto-differention\n",
    "- Facilitate distributed computation, spread the work across multiple CPUs, GPUs, or devices\n",
    "- Many common machine learning models are commonly taught and visualized as directed graphs already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "x = tf.add(a, b)\n",
    "with tf.Session() as sess:\n",
    "    # To activate TensorBoard on this program, add a line after we've built the graph, right before running the train loop\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)  # './graphs' is the logs_dir\n",
    "    print sess.run(x)\n",
    "\n",
    "writer.close() # close the writer when you'are done using it\n",
    "\n",
    "# Next, go to Terminal, run the program. Make sure that your present working directory is the same as where you ran your Python code.\n",
    "# python  [ yourprogram . py ]\n",
    "# tensorboard  --logdir = \"./graphs\"\n",
    "# Open your browser and go to http://localhost:6006/  (or the link you get back after running tensorboard command).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorflow Constant types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create constants of scalar or tensor values\n",
    "tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False)\n",
    "\n",
    "a = tf.constant([2, 2], name='vaector')\n",
    "b = tf.constant([[1, 2], [3, 4]], name='matrix')\n",
    "\n",
    "tf.zeros(shape, dtype=tf.float32, name=None)  # like numpy.zeros\n",
    "tf.zeros([2, 3], tf.int32)\n",
    "\n",
    "# like numpy.zeros_like, create a tensor of shape and type (unless type is specified) as the input_tensor but all elements are zeros.\n",
    "tf.zeros_like(tensor, dtype=None, name=None, optimize=True) # optimize: if true, attempt to statically determine the shape of 'tensor' and encode it as a constant.\n",
    "\n",
    "tf.ones(shape, dtype=tf.float32, name=None)  # like numpy.ones\n",
    "tf.ones_like(tensor, dtype=None, name=None, optimize=True) # like numpy.ones_like\n",
    "\n",
    "tf.fill(dims, value, name=None) # create a tensor filled with a scalar value.\n",
    "\n",
    "## Create constants that are sequences\n",
    "# Create a sequence of num evenly-spaced values are generated beginning at start. The values in the sequence increase by\n",
    "# (stop - start) / (num - 1). The last one is exactly stop\n",
    "tf.linspace(start, stop, num, name=None)\n",
    "\n",
    "# create a sequence of numbers that begins at start and extends by increments of delta up to but not including limit\n",
    "tf.range(start, limit=None, delta=1, dtype=None, name='range')\n",
    "\n",
    "## Tensorflow sequences are not iterable\n",
    "for  _  in  tf.linspace(0, 10, 4):   # TypeError(\"'Tensor' object is not iterable.\")\n",
    "for  _  in  tf.range(4):   # TypeError(\"'Tensor' object is not iterable.\")\n",
    "\n",
    "## Also could generate random constants from certain dist\n",
    "\n",
    "tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "# the values whose magnitude is more than 2 sd from the mean are dropped and re-picked\n",
    "tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)\n",
    "# randomly shuffle a tensor along its first dimension\n",
    "tf.random_shuffle(value, seed=None, name=None)\n",
    "# Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size.\n",
    "tf.random_crop(value, size, seed=None, name=None)\n",
    "# Draws samples from a multinomial distribution. logits: 2-D Tensor with shape [batch_size, num_classes]. Each slice [i, :] represents the unnormalized log probabilities for all classes.\n",
    "tf.multinomial(logits, num_samples, seed=None, name=None) \n",
    "tf.random_gamma(shape, alpha, beta=None, dtype=tf.float32, seed=None, name=None)\n",
    "tf.set_random_seed(seed) # set the graph-level random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Element-wise mathematical operations\n",
    "tf.add(x, y, name=None) # x, y: a tensor. Add supports broadcasting. AddN does not. \n",
    "tf.substract(x, y, name=None) # x, y: a tensor. supports broadcasting\n",
    "tf.multiply(x, y, name=None) # x, y: a tensor. supports broadcasting\n",
    "tf.scalar_mul(scalar, x) # x: A Tensor or IndexedSlices to be scaled.\n",
    "tf.div(x, y, name=None) # using Python2 division operator semantics. That is, if one of x or y is a float, then the result will be a float. Otherwise, the output will be an integer type. Prefer using the Tensor division operator or tf.divide which obey Python division operator semantics.using Python 2 division operator semantics. \n",
    "tf.divide(x, y, name=None) # python style division of x by y\n",
    "tf.truediv(x, y, name=None) # Python3 division operator semantics where all integer arguments are cast to floating types first.\n",
    "tf.floordiv(x, y, name=None) # Divides x / y elementwise, rounding toward the most negative integer.\n",
    "tf.realdiv(x, y, name=None) # If x and y are reals, this will return the floating-point division.\n",
    "tf.floor_div(x, y, name=None) # Returns x//y element-wise\n",
    "tf.truncateddiv(x, y, name=None) # Truncation designates that negative numbers will round fractional quantities toward zero. I.e. -7 / 5 = -1.\n",
    "tf.truncatemod(x, y, name=None) # reminder of division. C semantics in that the result here is consistent with a flooring divide. E.g. floor(x / y) * y + mod(x, y) = x.\n",
    "tf.floormod(x, y, name=None) or tf.floormod  # remainder of division. When x < 0 xor y < 0 is true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. floor(x / y) * y + mod(x, y) = x.\n",
    "...\n",
    "\n",
    "# Array Operations\n",
    "tf.add_n(inputs, name=None) # inputs: A list of Tensor objects, each with same shape and type.\n",
    "tf.Concat, Slice, Split, Constant, Rank, Shape, Shuffle\n",
    "...\n",
    "\n",
    "# Matrix Operations\n",
    "MatMul, MatrixInverse, MatrixDeterminant,...\n",
    "\n",
    "# Stateful Operations\n",
    "Variable, Assign, AssignAdd, ...\n",
    "\n",
    "# Neural Network Building Blocks\n",
    "Softmax, Sigmoid, ReLU, Convolution2D, MaxPool, ...\n",
    "\n",
    "# Checkpointing Operations\n",
    "Save, Restore\n",
    "\n",
    "# Queue and synchonization operations\n",
    "Enquene, Dequene, MutexAcquire, MutexRelease, ...\n",
    "\n",
    "# Control flow operations\n",
    "Merge, Switch, Enter, Leave, NextIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow takes in Python native types such as Python boolean values, numeric values (integers, floats), and strings. \n",
    "# Single values will be converted to 0-d tensors (or scalars), lists of values will be converted to 1-d tensors (vectors), \n",
    "# lists of lists of values will be converted to 2-d tensors (matrices), and so on. \n",
    "\n",
    "# https://www.tensorflow.org/programmers_guide/dims_types\n",
    "\n",
    "# NumPy Data Types\n",
    "# Could pass NumPy types to TensorFlow\n",
    "# Most of the times, you can use TensorFlow types and NumPy types interchangeably.\n",
    "tf.Session.Run(tf.ones([2, 2], np.float32))  ==>   numpy array [[1.0  1.0 ], [ 1.0   1.0 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A constant's value is stored in the graph and its value is replicated wherever the graph is loaded. \n",
    "# A variable is stored separately, and may live on a parameter server.\n",
    "\n",
    "# Declare variables\n",
    "# create variable a with scalar value\n",
    "a = tf.Variable(2, name=\"scalar\")\n",
    "# create variable b as a vector\n",
    "b = tf.Variable([2, 3], name=\"vector\")\n",
    "# create variable c as a 2x2 matrix\n",
    "c = tf.Variable([[0 ,  1], [2, 3]], name=\"matrix\")\n",
    "# create variable W as 784 x 10 tensor, filled with zeros\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# tf.Variable holds several ops:\n",
    "x.initializer # init \n",
    "x.value() # read op \n",
    "x.assign(...) # write op \n",
    "x.assign_add(...)\n",
    "# and more\n",
    "\n",
    "# You have to initialize variables before using them.  If you try to evaluate the variables before initializing them \n",
    "# you’ll run into FailedPreconditionError: Attempting to use uninitialized value tensor.\n",
    "\n",
    "# The easiest way is initializing all variables at once using: tf.global_variables_initializer()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess: \n",
    "    tf.run(init)\n",
    "    \n",
    "# To initialize only a subset of variables, you use tf.variables_initializer() \n",
    "# with a list of variables you want to initialize:\n",
    "\n",
    "init_ab = tf.variables_initializer([a, b], name=\"init_ab\")\n",
    "with tf.Session() as sess:\n",
    "    tf.run(init_ab)\n",
    "\n",
    "# You can also initialize each variable separately using tf.Variable.initializer\n",
    "# create variable W as 784 x 10 tensor, filled with zeros\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "with tf.Session() as sess:\n",
    "    tf.run(W.initializer)\n",
    "    \n",
    "    \n",
    "\"\"\"Evaluate values of variables\"\"\"\n",
    "# If we print the initialized variable, we only see the tensor object.\n",
    "# To get the value of a variable, we need to evaluate it using eval()\n",
    "W = tf.Variable(tf.truncated_normal([700, 10]))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(W.initializer)\n",
    "    print W.eval()\n",
    "    \n",
    "# Assign values to variables\n",
    "# We can assign a value to a variable using tf.Variable.assign()\n",
    "W = tf.Variable(10)\n",
    "W.assign(100) # W.assign(100) doesn’t assign the value 100 to W, but instead create an assign op to do that. For this op to take effect, we have to run this op in session.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(W.initializer)\n",
    "    print W.eval() # >> 10\n",
    "    \n",
    "W = tf.Variable(10)\n",
    "assign_op = W.assign(100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(assign_op) # don't have to init W as init ops is actually a assign ops, assigning the init value\n",
    "    print W.eval() # >> 100\n",
    "\n",
    "# create a variable whose original value is 2\n",
    "a = tf.Variable(2, name=\"scalar\")\n",
    "# assign a * 2 to a and call that op a_times_two\n",
    "a_times_two = a.assign(a * 2)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "# have to initialize a, because a_times_two op depends on the value of a \n",
    "sess.run(a_times_two) # >> 4\n",
    "sess.run(a_times_two) # >> 8\n",
    "sess.run(a_times_two) # >> 16\n",
    "\n",
    "# For simple incrementing and decrementing of variables, TensorFlow includes the \n",
    "# tf.Variable.assign_add() and tf.Variable.assign_sub() methods. \n",
    "# Unlike tf.Variable.assign(), tf.Variable.assign_add() and tf.Variable.assign_sub() \n",
    "# don’t initialize your variables for you because these ops depend on the initial values of the variable.\n",
    "W = tf.Variable(10)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(W.initializer)\n",
    "    print sess.run(W.assign_add(10)) # >> 20 \n",
    "    print sess.run(W.assign_sub(2)) # >> 18\n",
    "    \n",
    "# Because TensorFlow sessions maintain values separately, each Session can have its own current value for a variable defined in a graph.\n",
    "W = tf.Variable(10)\n",
    "sess1 = tf.Session() sess2 = tf.Session()\n",
    "sess1.run(W.initializer) sess2.run(W.initializer)\n",
    "print sess1.run(W.assign_add(10)) # >> 20 print sess2.run(W.assign_sub(2)) # >> 8\n",
    "print sess1.run(W.assign_add(100)) # >> 120 print sess2.run(W.assign_sub(50)) # >> -42\n",
    "sess1.close()\n",
    "sess2.close()\n",
    "\n",
    "# can declare a variable that depends on other variables.\n",
    "W = tf.Variable(tf.truncated_normal([700, 10]))\n",
    "U = tf.Variable(W * 2) #  should use initialized_value() to make sure that W is initialized before its value is used to initialize W.\n",
    "U = tf.Variable(W.intialized_value() * 2) # Not sure whether it's a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The only difference is an InteractiveSession makes itself the default session so you can call run() or eval() without explicitly call the session.\n",
    "sess = tf.InteractiveSession()\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "# We can just use 'c.eval()' without passing 'sess'\n",
    "print(c.eval()) \n",
    "sess.close()\n",
    "\n",
    "# tf.get_default_session() returns the default session for the current thread. The returned Session will be the innermost session \n",
    "# on which a Session or Session.as_default() context has been entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders and feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With the graph assembled, we, or our clients, can later supply their own data when they need to execute the computation.\n",
    "# Define a placeholder\n",
    "# Shape specifies the shape of the tensor that can be accepted as actual value for the placeholder. \n",
    "# shape=None means that tensors of any shape will be accepted. Using shape=None is easy to construct graphs, \n",
    "# but nightmarish for debugging. You should always define the shape of your placeholders as detailed as possible.\n",
    "tf.placeholder(dtype, shape=None, name=None)\n",
    "\n",
    "# create a placeholder of type float 32-bit, shape is a vector of 3 elements\n",
    "a = tf.placeholder(tf.float32, shape =[3])\n",
    "# create a constant of type float 32-bit, shape is a vector of 3 elements\n",
    "b = tf.constant([5, 5, 5], tf.float32)\n",
    "# use the placeholder as you would a constant or a variable\n",
    "c = a + b  # Short for tf.add(a, b)\n",
    "\"\"\"If  we  try  to fetch c ,  we will run  into  error.\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    ">>   NameError # because to compute c, we need the value of a, but a is just a placeholder without actual value. We have to first feed actual value into a.\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # feed [1, 2, 3] to placeholder a via the dict {a: [1, 2, 3]}\n",
    "    # fetch value of c\n",
    "    print(sess.run(c, {a: [1, 2, 3]}))\n",
    "\n",
    "# We can feed as any data points to the placeholder as we want by iterating through the data set and feed in the value one at a time.\n",
    "with tf.Session() as sess:\n",
    "    for a_value in list_of_a_values:\n",
    "        print (sess.run(c, {a: a_value}))\n",
    "        \n",
    "# You can feed values to tensors that aren’t placeholders. Any tensors that are feedable can be fed. To check if a tensor is feedable or not, use:\n",
    "tf.Graph.is_feedable(tensor)\n",
    "\n",
    "# create Operations, Tensors, etc (using the default graph)\n",
    "a = tf.add(2, 5)\n",
    "b = tf.mul(a, 3)\n",
    "# start up a `Session` using the default graph \n",
    "sess = tf.Session()\n",
    "# define a dictionary that says to replace the value of `a` with 15 \n",
    "replace_dict = {a: 15}\n",
    "# Run the session, passing in `replace_dict` as the value to `feed_dict` \n",
    "sess.run(b, feed_dict=replace_dict) # returns 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The trap of lazy loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Lazy loading is a term that refers to a programming pattern when you defer declaring/initializing an object until it is loaded. In the context of\n",
    "# TensorFlow, it means you defer creating an op until you need to compute it.\n",
    "# correct one\n",
    "x = tf.Variable(10, name='x')\n",
    "y = tf.Variable(20, name='y')\n",
    "z = tf.add(x, y)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(10):\n",
    "        sess.run(z)\n",
    "    writer.close()\n",
    "\n",
    "# not good one\n",
    "x = tf.Variable(10, name='x')\n",
    "y = tf.Variable(20, name='y')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(10):\n",
    "        sess.run(tf.add(x, y)) # create the op add only when you need to compute it \n",
    "    writer.close()\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
