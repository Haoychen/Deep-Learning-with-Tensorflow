{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queues and Coordinators\n",
    "Queues: important TensorFlow objects for computing tensors asynchronously in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In input pipeline, multiple threads can help us reduce the bottleneck at the reading in data phase \n",
    "# because reading in data is a lot of waiting.\n",
    "# in using queues to prepare inputs for training a model, we have:\n",
    "# Multiple threads prepare training examples and push them in the queue.\n",
    "# A training thread executes a training op that dequeues mini-batches from the queue.\n",
    "\n",
    "# The TensorFlow Session object is designed multithreaded, so multiple threads can easily use the same session \n",
    "# and run ops in parallel.\n",
    "# In python, All threads must be able to stop together, exceptions must be caught and reported, \n",
    "# and queues must be properly closed when stopping.\n",
    "\n",
    "# TensorFlow provides two classes to help with the threading: tf.Coordinator and tf.train.QueueRunner.\n",
    "# The Coordinator class helps multiple threads stop together and report exceptions to a program \n",
    "# that waits for them to stop. The QueueRunner class is used to create a number of threads \n",
    "# cooperating to enqueue tensors in the same queue.\n",
    "\n",
    "# two main queue classes, tf.FIFOQueue and tf.RandomShuffleQueue.\n",
    "# FIFOQueue creates a queue the dequeues elements in a first in first out order, \n",
    "# while RandomShuffleQueue dequeues elements in, well, a random order.\n",
    "# These two queues support the enqueue, enqueue_many, and dequeue\n",
    "\n",
    "# A common practice is that you enqueue many examples in when you read your data, but dequeue them one by one.\n",
    "# If you want to get multiple elements at once for your batch training,\n",
    "# you’ll have to use tf.train.batch or tf.train.shuffle_batch if you want to your batch to be shuffled.\n",
    "\n",
    "# There is also tf.PaddingFIFOQueue which is a FIFOQueue that supports batching variable-sized tensors by padding.\n",
    "# tf.PriorityQueue, which is a FIFOQueue whose enqueues and dequeues take in another argument: the priority\n",
    "\n",
    "# in practice, you rarely use a queue by itself, but always with string_input_producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "NUM_THREADS = 4\n",
    "# Generating some simple data\n",
    "# create 1000 random samples, each is a 1D array from the normal distribution (10, 1) \n",
    "data = 10 * np.random.randn(N_SAMPLES, 4) + 1\n",
    "# create 1000 random labels of 0 and 1\n",
    "target = np.random.randint(0, 2, size=N_SAMPLES)\n",
    "queue = tf.FIFOQueue(capacity=50, dtypes=[tf.float32, tf.int32], shapes=[[4], []])\n",
    "enqueue_op = queue.enqueue_many([data, target])\n",
    "dequeue_op = queue.dequeue()\n",
    "# create NUM_THREADS to do enqueue\n",
    "qr = tf.train.QueueRunner(queue, [enqueue_op] * NUM_THREADS)\n",
    "with tf.Session() as sess:\n",
    "# Create a coordinator, launch the queue runner threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    enqueue_threads = qr.create_threads(sess, coord=coord, start=True) \n",
    "    for step in xrange(100):   # do to 100 iterations\n",
    "        if coord.should_stop(): \n",
    "            break\n",
    "        data_batch, label_batch = sess.run(dequeue_op) \n",
    "    coord.request_stop()\n",
    "    coord.join(enqueue_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.Coordinator is used to manage threads of any thread we create, e.g., use python threading package to create threads\n",
    "import threading\n",
    "# thread body: loop until the coordinator indicates a stop was requested.\n",
    "# if some condition becomes true, ask the coordinator to stop.\n",
    "def my_loop(coord):\n",
    "    while not coord.should_stop():\n",
    "        ... do  something ... \n",
    "        if   ... some condition ...:\n",
    "            coord.request_stop() \n",
    "\n",
    "# main code: create a coordinator.\n",
    "coord = tf.Coordinator()\n",
    "# create 10 threads that run 'my_loop()'\n",
    "# you can also create threads using QueueRunner as the example above\n",
    "threads = [threading.Thread(target=my_loop, args=(coord,)) for _ in xrange(10)]\n",
    "# start the threads and wait for all of them to stop.\n",
    "for t in threads:\n",
    "    t.start()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.TextLineReader\n",
    "# output the lines of a file delimited by newlines e.g. text files, CSV files\n",
    "\n",
    "tf.FixedLengthRecoredReader\n",
    "# output the entire file when all files have same fixed lenghts\n",
    "# e.g. each MNIST file has 28 * 28 pixels, CIFAR-10 32 * 32 * 3\n",
    "\n",
    "tf.WholeFileReader\n",
    "# output the entire file content\n",
    "\n",
    "tf.TFRecordReader\n",
    "# reads samples from tensorflow's own binary format (TFRecords)\n",
    "\n",
    "tf.ReaderBase\n",
    "# Allows you to create your own readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To use data reader, we first need to create a queue to hold the names of all the files \n",
    "# that you want to read in through tf.train.string_input_producer.\n",
    "filename_queue = tf.train.string_input_producer([\"heart.csv\"])\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "# it means you choose to skip the first line for every file in the queue\n",
    "# think of readers as ops that return a different value every time you call it \n",
    "# it’ll return you a pair key, value, in which key is a key to identify the file and record \n",
    "# (useful for debugging if you have some weird records), and a scalar string value\n",
    "key, value = reader.read(filename_queue)\n",
    "# e.g.:\n",
    "key = data/heart.csv:2 # 2nd line in the file heart.csv\n",
    "value = 144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\n",
    "\n",
    "# tf.train.string_input_producer creates a FIFOQueue under the hood, so to run the queue, \n",
    "# we’ll need tf.Coordinator and tf.QueueRunner.\n",
    "filename_queue = tf.train.string_input_producer(filenames)\n",
    "reader = tf.TextLineReader(skip_header_lines=1) # skip the first line in the file \n",
    "key, value = reader.read(filename_queue)\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    print sess.run(key) # data/heart.csv:2\n",
    "    print sess.run(value) # 144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "# To convert string tensor into a vector representation of features\n",
    "# we need Tensorflow CSV decoder\n",
    "content = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "#  The record defaults serve two purposes:\n",
    "# 1. it tells the decoder what types of data to expect in each column.\n",
    "# 2.  if a space in a column happens to be empty, it’ll fill in that space \n",
    "#     with the default value of the data type that we specify.\n",
    "\n",
    "record_defaults = [[1.0] for _ in range(N_FEATURES)] # define all features to be floats\n",
    "record_defaults[4] = [''] # make the fifth feature string \n",
    "record_defaults.append([1]) # specify the 10th column to be integer, because we like our labels to be integer\n",
    "content = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "# You can also do all the kind of pre-processing you need for your data before feeding it in.\n",
    "# For example, now we have our content is a list of 10 elements, 8 are floats, 1 is string, and 1 is integer. \n",
    "# We’ll have to convert the string to float (Absent as 0 and Present as 1), \n",
    "# and then convert the first 9 features into a tensor that can be fed into the model.\n",
    "\n",
    "# convert the 5th column (present/absent) to the binary value 0 and 1\n",
    "condition = tf.equal(content[4], tf.constant('Present'))\n",
    "content[4] = tf.select(condition, tf.constant(1.0), tf.constant(0.0))\n",
    "# pack all 9 features into a tensor\n",
    "features = tf.pack(content[:N_FEATURES])\n",
    "# assign the last column to label\n",
    "label = content[-1]\n",
    "\n",
    "# Now each sample read from csv file would be converted into tensors\n",
    "# But we want to batch'em up. can do so using tf.train.batch, or tf.train.shuffle_batch \n",
    "# if you want to shuffle your batches.\n",
    "\n",
    "# minimum number elements in the queue after a dequeue, used to ensure # that the samples are sufficiently mixed\n",
    "# I think 10 times the BATCH_SIZE is sufficient\n",
    "min_after_dequeue = 10 * BATCH_SIZE\n",
    "# the maximum number of elements in the queue\n",
    "capacity =  20 * BATCH_SIZE\n",
    "# shuffle the data to generate BATCH_SIZE sample pairs\n",
    "data_batch, label_batch = tf.train.shuffle_batch([features, label], batch_size=BATCH_SIZE, capacity=capacity,\n",
    "                                                 min_after_dequeue=min_after_dequeue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord: TensorFlow's own binary data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# E.g. convert an image to a TFRecord\n",
    "def get_image_binary(filename):\n",
    "    image = Image.open(filename)\n",
    "    image = np.asarray(image, np.uint8)\n",
    "    shape = np.array(image.shape, np.int32)\n",
    "    return shape.tobytes(), image.tobytes()  # convert image to raw data bytes in the array.\n",
    "# Next, you write these byte strings into a TFRecord file using tf.python_io.TFRecordWriter and tf.train.Features.\n",
    "# You need the shape information so you can reconstruct the image from the binary format later.\n",
    "\n",
    "def write_to_tfrecord(label, shape, binary_image, tfrecord_file):\n",
    "    \"\"\"This example is to write a sample to TFRecord file. If you want to write more samples, just use loop\"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(tfrecord_file)\n",
    "    # write label, shape, and image content to the TFRecord file\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'label': tf.train.Feature(bytes_list=tf.train.BytesList(value=[label])),\n",
    "        'shape': tf.train.Feature(bytes_list=tf.train.BytesList(value=[shape])),\n",
    "        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[binary_image]))\n",
    "    }))\n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n",
    "# TO read a TFRecord file\n",
    "                       \n",
    "def read_from_tfrecord(filenames):\n",
    "    tfrecord_file_queue = tf.train.string_input_producer(filenames, name='queue')\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "    # label and image are stored as bytes but could be stored as\n",
    "    # int64 or float64 values in a serialized tf.Example protobuf.\n",
    "    tfrecord_features = tf.parse_single_example(tfrecord_serialized,\n",
    "                        features={\n",
    "                         'label': tf.FixedLenFeature([], tf.string ),\n",
    "                         'shape': tf.FixedLenFeature([], tf.string ),\n",
    "                         'image': tf.FixedLenFeature([], tf.string),\n",
    "                    }, name='features')              \n",
    "    # image was saved as uint8, so we have to decode as uint8.\n",
    "    image = tf.decode_raw(tfrecord_features['image'], tf.uint8)\n",
    "    shape = tf.decode_raw(tfrecord_features['shape'], tf.int32)\n",
    "    # The image tensor is flattened out, so we have to reconstruct the shape\n",
    "    image = tf.reshape(image, shape)\n",
    "    label = tf.cast(tfrecord_features['label'], tf.string)\n",
    "    return  label, shape, image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
