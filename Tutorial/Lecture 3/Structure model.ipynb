{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a one hidden layer neural network. The weights are the \"word vectors\"  \n",
    "2 training methods: hierachical softmax and negative sampling, would implement nagtive sampling here  \n",
    "Negative sampling is a simplified model of Noise Contrastive Estimation (NCE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How to structure tensorflow model*. \n",
    "Phase 1: assemble your graph\n",
    "1. Define placeholders for input and output\n",
    "2. Define the weights\n",
    "3. Define the inference model\n",
    "4. Define loss function\n",
    "5. Define optimizer\n",
    "\n",
    "Phase 2: execute the computation (training the model)\n",
    "1. Initialize all model vaiables for the first time\n",
    "2. Feed in the training data. Might involve randomizing the order of data samples\n",
    "3. Execute the inference model on training data, so it calculates for each training input example the output with the current model parameters\n",
    "4. Compute the cost\n",
    "5. Adjust the model parameters to minimize/maximize the cost depending on the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Phase 1\n",
    "# 1. define the placeholders\n",
    "# Input is the center word and output is the target (context) word. Instead of using one-hot vectors, we input the index of those words directly.\n",
    "center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "\n",
    "# 2. Define the weight\n",
    "embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0))\n",
    "\n",
    "# 3. Inference (compute the forward path of the graph)\n",
    "# each row of embed_matrix corresponds to the vector representation of the word at that index.\n",
    "#  So to get the representation of all the center words in the batch, we get the slice of all corresponding rows in the embedding matrix.\n",
    "# tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)\n",
    "embed = tf.nn.embedding_lookup(embed_matrix, center_words)\n",
    "\n",
    "# 4. Define the loss function\n",
    "# tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy='mod', name='nce_loss')\n",
    "# acutually in word2vec, the thrid argument is actually inputs and the fourth is labels\n",
    "# for nce_loss, we need weights and biases for the hidden layer to calculate nce loss\n",
    "nce_weight = tf.Variable(tf.truncated_norm([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / EMBED_SIZE ** 0.5))\n",
    "nce_bias = tf.Variable(tf.zeros[VOCAB_SIZE])\n",
    "loss = tf.reducemean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                    biases=nce_bias,\n",
    "                                    labels=target_words,\n",
    "                                    input=embed,\n",
    "                                    num_sampled=NUM_SAMPLED,\n",
    "                                    num_classes=VOCAL_SIZE))\n",
    "# 5. Define optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "# Phase 2.\n",
    "with tf.Session() as sess:\n",
    "    sess . run ( tf . global_variables_initializer ())\n",
    "    average_loss = 0.0\n",
    "    for index in xrange(NUM_TRAIN_STEPS):\n",
    "        batch = batch_gen.next()\n",
    "        loss_batch, _ = sess.run([loss, optimizer],\n",
    "                                feed_dict={center_words: batch[0], target_words: batch[1]})\n",
    "        average_loss += loss_batch\n",
    "        if (index + 1) % 2000 == 0:\n",
    "            print ('Average loss at step {}: {:5.1f}'.format(index + 1, average_loss / (index + 1)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Above definition of the model would lead the nodes in tensorboard scateering all over\n",
    "# Should tell tensorboard to know which nodes should be grouped\n",
    "with tf.name_scope(name_of_that_scope):\n",
    "    # declare op_1\n",
    "    # declare op_2\n",
    "    # ...\n",
    "\n",
    "    \n",
    "with tf.name_scope('data'):\n",
    "    center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name='center_words')\n",
    "    target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1], name='target_words')\n",
    "\n",
    "    \n",
    "with tf.name_scope('embed'):\n",
    "    embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name='embed_matrix')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "    nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / math.sqrt(EMBED_SIZE)), name='nce_weight')\n",
    "    nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    \"\"\"Build the graph for word2vec model\"\"\"\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        \"\"\"Step 1: Define the placeholders for input and output\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        \"\"\"Step 2: define weights.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        \"\"\"Step 3 + 4: define the inference and loss function\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"Step 5: define optimizer\"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "# obtain the embedding_matrix after you’ve trained it\n",
    "final_embed_matrix = sess.run(model.embed_matrix)\n",
    "\n",
    "# create a variable to hold your embeddings. It has to be a variable. Constants\n",
    "# don’t work. You also can’t just use the embed_matrix we defined earlier for our model. Why\n",
    "# is that so? I don't know. I get the 500 most popular words\n",
    "embedding_var = tf.Variable(final_embed_matrix[:500], name=\"embedding\")\n",
    "sess.run(embedding_var.initializer)\n",
    "config = projector.ProjectorConfig()\n",
    "summary_writer = tf.summary.FileWriter(LOGDIR)\n",
    "\n",
    "# add embeddings to config\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    " \n",
    "# link the embeddings to their metadata file. In this case, the file taht contains\n",
    "# the most 500 most popular words in our vocabulary\n",
    "embedding.metadata_path = LOGDIR + '/vocab_500.tsv'\n",
    "\n",
    "# save a configuration file that TensorBoard will read during startup\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "# save our embedding\n",
    "saver_embed = tf.train.Saver([embedding_var])\n",
    "saver_embed.save(sess, LOGDIR + '/skip-gram.ckpt', 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
